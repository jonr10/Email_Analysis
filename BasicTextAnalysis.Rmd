---
title: "Email_Analysis"
author: "Jonathan Roberts"
date: "7 April 2017"
output:
  html_notebook: default
  html_document: default
---

##R basics
###Global RMarkdown Parameters

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


###Install the R packages and libraries

```{r message = FALSE, warning = TRUE, include=FALSE}
libraries <- c("rmarkdown", "tidytext", "magrittr", "dplyr", "tidyr", "lazyeval", "purrr","ggplot2", "wordcloud", "reshape2")

#UNCOMMENT TO INSTALL PACKAGES
#lapply(libraries, install.packages)
lapply(libraries, library, character.only = TRUE)
```

### Working Directory: 
Set up where you are working and what is in the directory and what objects you have
 
```{r include=FALSE}

# SET THE WORKING DIRECTORY
setwd("/Users/jonathanroberts/Documents/R/Email_Analysis")
#dir()
#ls()

```



#Import data and write functions

I've put the data into a separate folder so that i don't git it - i don't want to push my work emails to a public site!!!

##Write in the data and have a look at it

This bit pulls the data and renames a few key columns. This code should work with the default way that outlook pushes data out to .csv files BUT i have not done any checking or error trapping so this will break and/or give odd results if data comes out from Outlook in a different format.

```{r include=FALSE}
#this is the csv that outlook spits out through it's export function. 
raw_data <- read.csv("../SensitiveData/2017_sent_emails.CSV",  colClasses = "character",stringsAsFactors = F)
#this is a list of stopwords specific to my emails, e.g. things that appear in my signature.
email_stopwords <- read.csv("../SensitiveData/email_stopwords.CSV", colClasses = "character")

#Have a look at the data
class(raw_data)
class(email_stopwords)

#rename the columns of the text and recipients so that goes into functions further down
colnames(raw_data)
colnames(raw_data)[2]<-"email"
colnames(raw_data)[6]<-"who"
colnames(raw_data)

raw_data[,6]
class(raw_data[,6])

#is.na(raw_data$Body)

```


##Function to Pull out relevant subsets of the data
This function takes a name of a person and returns all the rows in the data frame where there is a 'rough' match in the people you sent emails to.

This function needs your data to have a 'who' column that it 'filters' on the name you provide.

The search uses grepl which is not that robust for this purpose. It is o.k. though as by forcing to lower case and using \\b then you remove the sam -> Osama problem. I have introduced the Sam !-> Samuel problem though so perhaps remove the second \\b. Or just do some work on splitting out all the emails addresses.

```{r}

#i've design this function to only accept one name and call it further down as part of a for function so i can give it lots of people to analyse at once. I guess this bit could accept a character vector.

##TODO: this needs to be more robust to loewer case etc

individual_emails<-function(source_data, who = "Ross"){
        
        #vector to take the instances where the match is true
        who<-paste0("\\b",tolower(who),"\\b")
        v<-grepl(who, tolower(source_data$who))
        #filter on matches
        person_data<-(source_data[v,])
        return(person_data)
}

#sam<-individual_emails(raw_data, who="Sam")

```


## Function to Calculate common words and bigrams
This function gives the most common single words or bigrams not including stopwords.
It takes a dataframe as an argument, which must have a column called "Subject" that is
the text of interest, and one called "who"" that looks at who you are sending to.

Note: by going through a changing the two places "subject" is written with "emails" it will do the analysis on the text of the emails instead of the subject line. 

```{r}
## TODO: i want to be able to call to $email or $Subject in the defintion of the function, but can't do this in the obvious way.

mostcommon <- function(text_df,n=1,x=20) {
  if(n==1){
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,"Subject",to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
    #count the occurrences of each word, sort by the number of occurrences, and take the top x
    top_x <- (clean_Qdf %>% count(word,sort=TRUE))[1:x,]
  }
  else if(n==2){
    #manipulate the data so that each bigram has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(bigram,"Subject",to_lower=TRUE,token="ngrams",n=2)
    #separate bigrams into individual words
    bigrams_separated <- tidy_Qdf %>% separate(bigram, c("word1", "word2"), sep = " ")
    #remove cases where one of the words is a stopword
    bigrams_filtered <- bigrams_separated %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
    
    #count the occurrences of word pairs, sort by the number of occurrences, and take the top x
    top_x <- as.data.frame((bigrams_filtered %>% count(word1, word2, sort = TRUE))[1:x,])
    
    #rejoin the words back into bigrams
    top_x$phrase <- sapply(1:x,
                           function(x)
                             paste(top_x[x,]$word1,top_x[x,]$word2))
    #only keep the bigrams
    top_x <- top_x[,!(names(top_x) %in% c("word1","word2"))]
  }
  who<-rep(text_df$who[1],x)
  return(cbind(top_x,who))
}


```



## Function to Make Tidy text data
Tweaking the approach above so we pull out tidy data with stop words removed from a function then after that do stuff to it

TODO: for some reason this function doesn't allow you to pass "email" or "Subject" as an argument and work in the way that you would expect. 

```{r}

tidy_stop_email <- function(text_df) {
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,"email",to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
}

tidy_stop_subject <- function(text_df) {
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,"Subject",to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
}


```

#Text Analysis

##On the whole corpus

###WordCloud all the email text
```{r}

email_text<-tidy_stop_email(raw_data)
email_text %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60))

```

###Sentiment wordcloud all the email text
It would be rude not to...

```{r}

bing <- get_sentiments("bing")

email_text %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 60)

```

###Look at different emotions using nrc
```{r}
nrc <- get_sentiments("nrc")
nrc_word_counts <- email_text %>%
  inner_join(nrc) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

nrc_word_counts

```


###WordCloud all the Subject line text
```{r}
subj_text<-tidy_stop_subject(raw_data)
subj_text %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 60))

```





##Analysis on one person
Drop their name into the function call, or leave blank for Ross...

```{r}
# Call the Person (s) that you want
input_data<-individual_emails(raw_data, "rahman")
#Make the tidy data using unnest and removing stop words

ind_email_text<-tidy_stop_email(input_data)

top_20 <- (ind_email_text %>% count(word,sort=TRUE))[1:20,]
```


###Wordclouds
Well why not - it is text analytics after all...

```{r}

ind_email_text %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 80))

```


###Sentiment wordcloud
It would be rude not to...

```{r}

bing <- get_sentiments("bing")

ind_email_text %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)

```


###Some sentiment analysis

Select a particular 'emotion' in the nrc library
```{r}

nrc <- get_sentiments("nrc") %>%  filter(sentiment == "trust")

email_text%>%
  semi_join(nrc) %>%
  count(word, sort = TRUE)


```

Next look at positive and negative using the bing library

```{r}
bing <- get_sentiments("bing")
bing_word_counts <- email_text %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts

```


## Calling BiGrams and writing out data

Write the name(s) of the people who's email you want to analyse, then it calls the functions and pushes the results out to .csv files.

TODO: The rbind process is inefficient accroding to Robin, so probably want to do this is a slightly different way to speed up in due course.

NOTE: The code requires you to input whose name you are looking for, this could be in an interface in future.


```{r}

####Actual work####

#Set up a vector of people you want to look for in your email.
sent_to<-c("Tazzyman", "Wyatt", "Marriott")

# loop around all the people calling a function to filter relevant emails then 
# the cbind here binds the input search term to what you get back from the function 'mostcommon'
# which itself includes the some information on who you've been emailing too.

for (i in 1:length(sent_to)){
        
        if (i==1){ 
                input_data<-individual_emails(raw_data, who=sent_to[i])
                #commonWords <- input_data %>% mostcommon() %>% cbind(sent_to[i])
                commonBigrams <- input_data %>% mostcommon(n=2) %>% cbind(sent_to[i])
        }
        else {
                input_data<-individual_emails(raw_data, who=sent_to[i])
                #commonWords <- input_data %>% mostcommon() %>% cbind(sent_to[i]) %>% rbind(commonWords)
                commonBigrams <- input_data %>% mostcommon(n=2) %>% cbind(sent_to[i]) %>% rbind(commonBigrams)
        }
}


#and write out
#write.csv(commonWords,"../SensitiveData/commonWords.csv")
write.csv(commonBigrams,"../SensitiveData/commonBigrams.csv")


```



##Plot bigrams

This only plots for one person. 
TODO: Add functionality that plots for more than one person. Could do by splitting and selecting one person, or i guess by multiple charts...

```{r}

#plot(commonBigrams$`sent_to[i]`, commonBigrams$n)
#Well barplot is a bit sh1t. 
#barplot(commonBigrams$n, names.arg = commonBigrams$phrase, horiz = TRUE)


ggplot(commonBigrams, aes(x = phrase, y = n, fill = who)) + geom_bar(stat = "identity", show.legend = FALSE) +
  xlab("Terms") + ylab("Count") + coord_flip() +  facet_wrap(~who, ncol = 3, scales = "free_x")


#ggplot(janeaustensentiment, aes(index, sentiment, fill = book)) +
#  geom_bar(stat = "identity", show.legend = FALSE) +
#  facet_wrap(~book, ncol = 2, scales = "free_x")

```


#Analysing Outlook email data
##Short term aims
###Person by person
Shot term aim will be to have a function where you send it a person/email adress and it will anaylse your interaction with them: Probably by doing something like:
*Most popular words and biGrams
*sentiment analysis
*Topic Modelling

As part of this it would be useful to get basic lists of people that you email.

###Corpus
It might be interesting to do some LSA on all your emails and see if it draws out groups of people that you interact with, a different way of looking a network

*topic modelling and see how the changes over time
*LSA/clustering and see how that changes over time
Maybe doing that sort of thing when passing a set of users over to it

###Networks
Another area to explore might be linking to other people's data and having some network diagrams perhaps.

###jonbot
Ultimately i'd like to get something which might be a jonbot and suggest responses to emails for me...


##Analysing who you email
A lazy way to do this is to call for bigrams for the whole who column. 

