---
title: "Email_Analysis"
author: "Jonathan Roberts"
date: "7 April 2017"
output: 
    html_notebook: default
    html_document: default
    
---

#Get up and running
##Global RMarkdown Parameters
Are set here

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


##Install the R markdown packages and library

```{r message = FALSE, warning = TRUE}
libraries <- c("rmarkdown", "tidytext", "magrittr", "dplyr", "tidyr", "lazyeval", "purrr")

#UNCOMMENT TO INSTALL PACKAGES
lapply(libraries, install.packages)
lapply(libraries, library, character.only = TRUE)
```

## Working Directory: 
Set up where you are working and what is in the directory and what objects you have
 
```{r}

# SET THE WORKING DIRECTORY
setwd("/Users/jonathanroberts/Documents/R/Email_Analysis")
dir()
ls()

```

#Assignment: Part 1 & 2


#Playing Around with Email Data
Shot term aim will be to have a function where you send it a person/email adress and you can 
Long term aim might be to do:
topic modelling and see how the changes over time
LSA/clustering and see how that changes over time
Maybe doing that sort of thing when passing a set of users over to it

Another area to explore might be linking to other people's data and having some network diagrams perhaps.

Ultimately i'd like to get something which might be a jonbot and suggest responses to emails for me...



##Write in the data and have a look at it

```{r}

raw_data <- read.csv("../SensitiveData/2017_sent_emails.CSV", colClasses = "character")
email_stopwords <- read.csv("../SensitiveData/email_stopwords.CSV", colClasses = "character")

#Have a look at the data
class(raw_data)
class(email_stopwords)

#rename the columns of the text and recipients so that goes into functions further down
colnames(raw_data)
colnames(raw_data)[2]<-"email"
colnames(raw_data)[6]<-"who"
colnames(raw_data)

raw_data[,6]
class(raw_data[,6])

charmatch("Wyatt, Ross",raw_data[,6])

```


##Pull out relevant subsets of the data
Write a function that receives the rawdata and an email address and send back the raw data for that person, and possible just the email text and not all the rest of the data.

First time round probably do some loose search on whether they were one of the people emailed, maybe a function like contains - although this isn't an r thing.

```{r}

individual_emails<-function(source_data, text_column, name = "Mike", name_column){
  #take in the source_data
  #look for the person's name
  #filter the sourcedata on that name
  #return the text column
}

##uncomment the following line when ready to call the function
#text_to_analyse <- individual_emails(raw_data,2,5)

#TODO: note that charmatch doesn't work

```


## Calculate common words and bigrams
This function gives the most common single words or bigrams not including stopwords.
It takes a dataframe as an argument, which must have a column called "email" that is
the answers to the question of interest, and one called "who"" that looks at who you are sending to.


```{r}


###HELPER FUNCTIONS###


mostcommon <- function(text_df,n=1,x=20){
  if(n==1){
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,email,to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
    #count the occurrences of each word, sort by the number of occurrences, and take the top x
    top_x <- (clean_Qdf %>% count(word,sort=TRUE))[1:x,]
  }
  else if(n==2){
    #manipulate the data so that each bigram has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(bigram,email,to_lower=TRUE,token="ngrams",n=2)
    #separate bigrams into individual words
    bigrams_separated <- tidy_Qdf %>% separate(bigram, c("word1", "word2"), sep = " ")
    #remove cases where one of the words is a stopword
    bigrams_filtered <- bigrams_separated %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
    
    #count the occurrences of word pairs, sort by the number of occurrences, and take the top x
    top_x <- as.data.frame((bigrams_filtered %>% count(word1, word2, sort = TRUE))[1:x,])
    
    #rejoin the words back into bigrams
    top_x$phrase <- sapply(1:x,
                           function(x)
                             paste(top_x[x,]$word1,top_x[x,]$word2))
    #only keep the bigrams
    top_x <- top_x[,!(names(top_x) %in% c("word1","word2"))]
  }
  #who<-rep(text_df$who,x)
  #return(cbind(top_x,who))
        return(top_x)
}


```


##Set the loop to go around who is emailing you

TODO: this is tricky as we haven't defined this yet - so i will not use the who information but just pull out the bigrams for the whole corpus.

```{r}

####Actual work####

#Now we start with the results for question 25 and then rbind iteratively
#through all the other questions 24 to 1, for both words and bigrams.
commonWords <- raw_data %>% mostcommon( x=50)
commonBigrams <- raw_data %>% mostcommon(n=2, x=50)

#and write out
write.csv(commonWords,"../SensitiveData/commonWords.csv")
write.csv(commonBigrams,"../SensitiveData/commonBigrams.csv")


```





