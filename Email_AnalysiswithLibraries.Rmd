---
title: "Email_Analysis"
author: "Jonathan Roberts"
date: "7 April 2017"
output:
  html_notebook: default
  html_document: default
---

#Get up and running
##Global RMarkdown Parameters
Are set here

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


##Install the R markdown packages and library

```{r message = FALSE, warning = TRUE, include=FALSE}
libraries <- c("rmarkdown", "tidytext", "magrittr", "dplyr", "tidyr", "lazyeval", "purrr","ggplot2")

#UNCOMMENT TO INSTALL PACKAGES
#lapply(libraries, install.packages)
lapply(libraries, library, character.only = TRUE)
```

## Working Directory: 
Set up where you are working and what is in the directory and what objects you have
 
```{r include=FALSE}

# SET THE WORKING DIRECTORY
setwd("/Users/jonathanroberts/Documents/R/Email_Analysis")
#dir()
#ls()

```


#Analysing Outlook email data
##Short term aims
###Person by person
Shot term aim will be to have a function where you send it a person/email adress and it will anaylse your interaction with them: Probably by doing something like:
*Most popular words and biGrams
*Topic Modelling

As part of this it would be useful to get basic lists of people that you email.

###Corpus
It might be interesting to do some LSA on all your emails and see if it draws out groups of people that you interact with, a different way of looking a network

*topic modelling and see how the changes over time
*LSA/clustering and see how that changes over time
Maybe doing that sort of thing when passing a set of users over to it

###Networks
Another area to explore might be linking to other people's data and having some network diagrams perhaps.

###jonbot
Ultimately i'd like to get something which might be a jonbot and suggest responses to emails for me...

#Some Code
##Write in the data and have a look at it
I've put the data into a separate folder so that i don't git it.
This bit pulls the data and renames a few key columns. This code should work with the default way that outlook pushes data out to .csv files BUT i have not done any checking or error trapping so this will break and/or give odd results if data comes out from Outlook in a different format.

```{r include=FALSE}
#this is the csv that outlook spits out through it's export function. 
raw_data <- read.csv("../SensitiveData/2017_sent_emails.CSV",  colClasses = "character",stringsAsFactors = F)
#this is a list of stopwords specific to my emails, e.g. things that appear in my signature.
email_stopwords <- read.csv("../SensitiveData/email_stopwords.CSV", colClasses = "character")

#Have a look at the data
class(raw_data)
class(email_stopwords)

#rename the columns of the text and recipients so that goes into functions further down
colnames(raw_data)
colnames(raw_data)[2]<-"email"
colnames(raw_data)[6]<-"who"
colnames(raw_data)

raw_data[,6]
class(raw_data[,6])

```

```{r include=FALSE}
is.na(raw_data$Body)
```



##Pull out relevant subsets of the data
This function takes a name of a person and returns all the rows in the data frame where there is a 'rough' match in the people you sent emails to.

This function needs your data to have a 'who' column that it 'filters' on the name you provide.

The search uses grepl which is not that robust for this purpose. for example 'sam' finds "Osama"" but not "Sam".

```{r}

#i've design this function to only accept one name and call it further down as part of a for function so i can give it lots of people to analyse at once. I guess this bit could accept a character vector.

##TODO: this needs to be more robust to loewer case etc

individual_emails<-function(source_data, who = "Ross"){
        
        #vector to take the instances where the match is true
        who<-paste0("\\b",tolower(who),"\\b")
        v<-grepl(who, tolower(source_data$who))
        #filter on matches
        person_data<-(source_data[v,])
        return(person_data)
}

sam<-individual_emails(raw_data, who="Sam")

```


## Calculate common words and bigrams
This function gives the most common single words or bigrams not including stopwords.
It takes a dataframe as an argument, which must have a column called "Subject" that is
the text of interest, and one called "who"" that looks at who you are sending to.

Note: by going through a changing the two places "subject" is written with "emails" it will do the analysis on the text of the emails instead of the subject line. 

```{r}

###HELPER FUNCTIONS###

## TODO: i want to be able to call to $email or $Subject in the defintion of the function, but can't do this in the obvious way.

mostcommon <- function(text_df,n=1,x=20) {
  if(n==1){
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,"Subject",to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
    #count the occurrences of each word, sort by the number of occurrences, and take the top x
    top_x <- (clean_Qdf %>% count(word,sort=TRUE))[1:x,]
  }
  else if(n==2){
    #manipulate the data so that each bigram has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(bigram,"Subject",to_lower=TRUE,token="ngrams",n=2)
    #separate bigrams into individual words
    bigrams_separated <- tidy_Qdf %>% separate(bigram, c("word1", "word2"), sep = " ")
    #remove cases where one of the words is a stopword
    bigrams_filtered <- bigrams_separated %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
    
    #count the occurrences of word pairs, sort by the number of occurrences, and take the top x
    top_x <- as.data.frame((bigrams_filtered %>% count(word1, word2, sort = TRUE))[1:x,])
    
    #rejoin the words back into bigrams
    top_x$phrase <- sapply(1:x,
                           function(x)
                             paste(top_x[x,]$word1,top_x[x,]$word2))
    #only keep the bigrams
    top_x <- top_x[,!(names(top_x) %in% c("word1","word2"))]
  }
  who<-rep(text_df$who[1],x)
  return(cbind(top_x,who))
}


```


##Call the functions to do the analysis for a vector of names
Write the name(s) of the people who's email you want to analyse, then it calls the functions and pushes the results out to .csv files.

TODO: The rbind process is inefficient accroding to Robin, so probably want to do this is a slightly different way to speed up in due course.

NOTE: The code requires you to input whose name you are looking for, this could be in an interface in future.


```{r}

####Actual work####

#Set up a vector of people you want to look for in your email.
sent_to<-c("Driver", "Rahman", "Marriott")

# loop around all the people calling a function to filter relevant emails then 
# send to commonWords & commonBigrams functions
for (i in 1:length(sent_to)){
        
        if (i==1){ 
                input_data<-individual_emails(raw_data, who=sent_to[i])
                commonWords <- input_data %>% mostcommon() %>% cbind(sent_to[i])
                commonBigrams <- input_data %>% mostcommon(n=2) %>% cbind(sent_to[i])
        }
        else {
                input_data<-individual_emails(raw_data, who=sent_to[i])
                commonWords <- input_data %>% mostcommon() %>% cbind(sent_to[i]) %>% rbind(commonWords)
                commonBigrams <- input_data %>% mostcommon(n=2) %>% cbind(sent_to[i]) %>% rbind(commonBigrams)
        }
}


#and write out
write.csv(commonWords,"../SensitiveData/commonWords.csv")
write.csv(commonBigrams,"../SensitiveData/commonBigrams.csv")


```

##Can we plot a few things please

This only plots for one person. 
TODO: Add functionality that plots for more than one person. Could do by splitting and selecting one person, or i guess by multiple charts...

```{r}

#plot(commonBigrams$`sent_to[i]`, commonBigrams$n)
#Well barplot is a bit sh1t. 
#barplot(commonBigrams$n, names.arg = commonBigrams$phrase, horiz = TRUE)


ggplot(commonBigrams, aes(x = phrase, y = n)) + geom_bar(stat = "identity") +
  xlab("Terms") + ylab("Count") + coord_flip()


```


##Analysing who you email
A lazy way to do this is to call for bigrams for the whole who column. 

```{r}
install.packages("data.table")
install.packages("tm")
install.packages("lsa")
install.packages("cluster")
install.packages("LSAfun")

library(data.table)
library(tm)
library(lsa)
library(cluster)
library(LSAfun)

```
# NLP: identifying similar emails.

This does the burdensome analysis needed to run Latent Semantic Analysis on the set of emails.
The set itself is generated above, and can be done per person(s) or for the whole corpus


##General code commenting: to be removed

```{r}


#This creates and saves the following R objects, so that they can be loaded in to an R script
#run in the background for Tableau:
# Latent Semantic Analysis space (saved as lsaOut.rda)
# Text-Document Matrix (saved as tdm.rda)
# Results of a clustering (saved as kluster.rda)

#It also saves two .csv files which are then directly loaded into Tableau as data:
# All the data for XXs, and the cluster that each belongs to (AllXXsForTableau.csv)
# The top dozen words for each cluster (topDozen.csv)

#To use, just write in the file name of the .csv containing the XXs and run the code.

```

##Set the source file


```{r}
#PARAMETERS
##TODO
file <- "../SensitiveData/2017_sent_emails.CSV"

```


##A Couple of functions

TODO: Could and should tidy up the cleaning approach, and add in email_stopwords. I have done this before so can steal that code.
```{r}

#FUNCTIONS

#a list of stopwords to be removed from the PQs to avoid false similarities on the grounds
#of questions containing these words
stopwordList <- c(stopwords())
##TODO: NEED TO ADD IN MY EMAIL STOP WORDS - JUST ADDING IN THE STOPWORDS VECTOR DOESN'T WORK, just needs to be the right syntx i expect

#a function to clean a corpus of text, making sure of the encoding, removing punctuation, putting it
#all in lower case, stripping white space, and removing stopwords.
cleanCorpus <- function(corp) {
  corp <-tm_map(corp, content_transformer(function(x) iconv(x, to='UTF-8-MAC', sub="byte")))
  toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, ' ', x))})
  corp <- tm_map(corp, toSpace, '-')
  corp <- tm_map(corp, toSpace, '’')
  corp <- tm_map(corp, toSpace, '‘')
  corp <- tm_map(corp, toSpace, '•')
  corp <- tm_map(corp, toSpace, '”')
  corp <- tm_map(corp, toSpace, '“')
  corp <- tm_map(corp,content_transformer(tolower))
  corp <- tm_map(corp,removePunctuation)
  corp <- tm_map(corp,stripWhitespace)
  corp <- tm_map(corp, function(x) removeWords(x,stopwordList))
}

#a function useful in debugging so you can read a given document in a given corpus easily
writeDoc <- function(num,corpus){
  writeLines(as.character(corpus$content[[num]]))
}

#a function to summarise the top terms of a given cluster
summarise <- function(clusterNum,matr,totalClusters,hierarchy,numTerms,listOfVectors){
  clusterSet <- cutree(hierarchy,totalClusters)
  relevantQs <- matr[,which(clusterSet==clusterNum)]
  clusterDict <- cleanCorpus(Corpus(VectorSource(listOfVectors[which(clusterSet==clusterNum)])))
  termsAndSums <- if(is.null(dim(relevantQs))){relevantQs} else rowSums(relevantQs)
  termsAndSumsN <- termsAndSums[order(termsAndSums,decreasing=T)[1:numTerms]]
  names(termsAndSumsN) <- stemCompletion(names(termsAndSumsN),clusterDict)
  termsAndSumsN
}

```

##Read in the data and clean out stopwords
TODO: This should be steamlined, basically set strings as factors above and don;t re-call the .csv
TODO: make the clean function neater and more robust

```{r}
#SCRIPT

#read in questions
raw_data <- read.csv(file, stringsAsFactors = F) #, colClasses = "character")
questionsVec <- raw_data$Subject
questionsVec[825]
#----is.na(questionsVec)
#make sure it's in utf-8 format
##When this introduces NA's then it breaks the standard hclust approach. 
##This introduces lots of NAs for emails and a few for the subject
questionsVec <- iconv(questionsVec,to="utf-8-mac", sub="byte")
#questionsVec <- enc2utf8(questionsVec)

#questionsVec[is.na(questionsVec)]
questionsVec<-questionsVec[1:35]
questionsVec


##TODO: this is a summer lazy and inelegant work around, not tenable in the long term as it strips out content
#questionsVec<-questionsVec[is.na(questionsVec)]
```

##Start making the corpus and doing the NLP

```{r}
#Create the corpus
PQCorp <- Corpus(VectorSource(questionsVec))
#Stem the corpus
PQCorp.stems <- tm_map(cleanCorpus(PQCorp),stemDocument)

class(PQCorp)

#Create the term-document matrix. For each term in each document we assign a score based on the
#inverse frequency of the appearance of that term in documents in the corpus, normalised for the
#document length (in some sense), and zero if the term is absent from the document entirely.
#Details can be seen by inspecting the help documentation for the weightSMART function.
tdm<-TermDocumentMatrix(PQCorp.stems,control =list(weighting = function(x) weightSMART(x, spec = "btc")))


#Creat the latent semantic space. The idea is that it creates a basis of variation, like a PCA, and
#allows you to cut down the number of dimensions you need. Here I've determined the number of dimensions
#such that all of them contribute an s-value of at least 1 (the 'Kaiser-Criterion').
lsaOut <- lsa(tdm,dims=dimcalc_kaiser())
#positions of our documents in this latent semantic space.
posns <-diag(lsaOut$sk) %*% t(lsaOut$dk)
#distances between documents in this space, based on cosine similarity.
diss <- 1-cosine(posns)

#a hierarchical clustering. At the moment we only use this to define our clusters,
#by taking a cut through it at the right stage. There is no doubt more that could
#be done using the hierarchy.
hier<-hclust(as.dist(diss),method = "complete")

#We choose 1000 to be the number of clusters into which we divide our set of questions.
#See the appendix for some sort of reasoning behind this.
k <- 20
klusters <- cutree(hier,k)
#this summarises the top 12 terms per cluster using the summarise function from above.
##TODO: we have no m again

topDozen <- data.frame(
  cluster=unlist(lapply(seq(1,k),function(x)rep(x,12))),
  word=unlist(lapply(seq(1,k),function(x) names(summarise(x,m,k,hier,12,questionsVec)))),
  freq=unlist(lapply(seq(1,k),function(x) summarise(x,m,k,hier,12,questionsVec))),
  row.names = NULL ,stringsAsFactors = F)

```

##save and output data
```{r}

#### SAVING ####

#Save the R output to be loaded in to R when Tableau is running
save(tdm, file='tdm.rda')
save(lsaOut,file='lsaOut.rda')
save(klusters,file='klusters.rda')

#Save data to be directly loaded in to Tableau

#The questions and their data (including cluster)
savedf <- data.frame(
  Question_ID = aPQ$Question_ID,
  Question_Text = aPQ$Question_Text,
  Answer_Text = aPQ$Answer_Text,
  Question_MP = aPQ$Question_MP,
  MP_Constituency = aPQ$MP_Constituency,
  Answer_MP = aPQ$Answer_MP,
  Date = aPQ$Date,
  Answer_Date = aPQ$Answer_Date,
  Corrected_Date = aPQ$Corrected_Date,
  Cluster = klusters,
  stringsAsFactors = FALSE)
write.csv(savedf,'MoJallPQsforTableau.csv')

#The information about the clusters
write.csv(topDozen,'topDozen.csv')


##### APPENDIX #####

#Here we see how many clusters is a good number for our data. We calculate the
#silhouette for each clustering - the higher the better. We also calculate the
#median number of questions per cluster given the total cluster number.
#If these calculations have already been done you can simply load the
#'silhouettewidths.rda' and 'medianpercluster.rda' files. Otherwise you will
#have to regenerate the value running the code.

load(file='silhouettewidths.rda')
load(file='medianpercluster.rda')

#if you want to regenerate the data run the following
#ksilwidths <- sapply(seq(2,4000), function(x) mean(silhouette(cutree(hier,x),distn)[,3]))
#if you want to save it
#save(ksilwidths,file='silhouettewidths.rda')
#medianNumPerCluster <- function(hierarch,k){
#  klusters <- cutree(hierarch,k)
#  median(sapply(seq(1,k), function(x) length(which(klusters == x))))
#}
#meds <- sapply(seq(2,4000),function(x) medianNumPerCluster(hier,x))
#save(meds,file='medianpercluster.rda')

plot(ksilwidths, type="l")
which.max(ksilwidths)
max(ksilwidths)
meds[which.max(ksilwidths)]
#you can see that the 'best' number of clusters is around 2668. However, this results in
#a median of only two questions per cluster, and the silhouette is still pretty small, at ~0.23.
#So we probably want more questions per cluster on average, particularly as it's not like the
#clusterings are 'good' anyway. Hence the arbitrary choice of 1000, which gives a silhouette
#of ~0.161 and a median of 4 questions per cluster.
ksilwidths[1000]
meds[1000]

#We might be able to do better than arbitrarily picking 1000 by defining some function of
#median and silhouette and maximising it (although then the function definition is still
#arbitary).


```



