---
title: "Email_Analysis"
author: "Jonathan Roberts"
date: "7 April 2017"
output: 
    html_notebook: default
    html_document: default
    
---

#Get up and running
##Global RMarkdown Parameters
Are set here

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)
```


##Install the R markdown packages and library

```{r message = FALSE, warning = TRUE}
libraries <- c("rmarkdown", "tidytext", "magrittr", "dplyr", "tidyr", "lazyeval", "purrr","ggplot")

#UNCOMMENT TO INSTALL PACKAGES
#lapply(libraries, install.packages)
lapply(libraries, library, character.only = TRUE)
```

## Working Directory: 
Set up where you are working and what is in the directory and what objects you have
 
```{r}

# SET THE WORKING DIRECTORY
setwd("/Users/jonathanroberts/Documents/R/Email_Analysis")
#dir()
#ls()

```


#Analysing Outlook email data
##Short term aims
###Person by person
Shot term aim will be to have a function where you send it a person/email adress and it will anaylse your interaction with them: Probably by doing something like:
*Most popular words and biGrams
*Topic Modelling

As part of this it would be useful to get basic lists of people that you email.

###Corpus
It might be interesting to do some LSA on all your emails and see if it draws out groups of people that you interact with, a different way of looking a network

*topic modelling and see how the changes over time
*LSA/clustering and see how that changes over time
Maybe doing that sort of thing when passing a set of users over to it

###Networks
Another area to explore might be linking to other people's data and having some network diagrams perhaps.

###jonbot
Ultimately i'd like to get something which might be a jonbot and suggest responses to emails for me...

#Some Code
##Write in the data and have a look at it
I've put the data into a separate folder so that i don't git it.
This bit pulls the data and renames a few key columns. This code should work with the default way that outlook pushes data out to .csv files BUT i have not done any checking or error trapping so this will break and/or give odd results if data comes out from Outlook in a different format.

```{r}
#this is the csv that outlook spits out through it's export function. 
raw_data <- read.csv("../SensitiveData/2017_sent_emails.CSV", colClasses = "character")
#this is a list of stopwords specific to my emails, e.g. things that appear in my signature.
email_stopwords <- read.csv("../SensitiveData/email_stopwords.CSV", colClasses = "character")

#Have a look at the data
class(raw_data)
class(email_stopwords)

#rename the columns of the text and recipients so that goes into functions further down
colnames(raw_data)
colnames(raw_data)[2]<-"email"
colnames(raw_data)[6]<-"who"
colnames(raw_data)


raw_data[,6]
class(raw_data[,6])

```


##Pull out relevant subsets of the data
This function takes a name of a person and returns all the rows in the data frame where there is a 'rough' match in the people you sent emails to.

This function needs your data to have a 'who' column that it 'filters' on the name you provide.

The search uses grepl which is not that robust for this purpose. for example 'sam' finds "Osama"" but not "Sam".

```{r}

#i've design this function to only accept one name and call it further down as part of a for function so i can give it lots of people to analyse at once. I guess this bit could accept a character vector.

##TODO: this needs to be more robust to loewer case etc

individual_emails<-function(source_data, who = "Ross"){
        
        #vector to take the instances where the match is true
        v<-grepl(who, source_data$who)
        #filter on matches
        person_data<-(source_data[v,])
        return(person_data)
}

sam<-individual_emails(raw_data, who="Mike")


```


## Calculate common words and bigrams
This function gives the most common single words or bigrams not including stopwords.
It takes a dataframe as an argument, which must have a column called "Subject" that is
the text of interest, and one called "who"" that looks at who you are sending to.

Note: by going through a changing the two places "subject" is written with "emails" it will do the analysis on the text of the emails instead of the subject line. 

```{r}

###HELPER FUNCTIONS###

## TODO: i want to be able to call to $email or $Subject in the defintion of the function, but can't do this in the obvious way.

mostcommon <- function(text_df,n=1,x=20){
  if(n==1){
    #manipulate the data so that each word has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(word,Subject,to_lower=TRUE)
    #remove stopwords
    clean_Qdf <- tidy_Qdf %>% anti_join(stop_words)  
    clean_Qdf <- clean_Qdf %>% anti_join(email_stopwords)
    #count the occurrences of each word, sort by the number of occurrences, and take the top x
    top_x <- (clean_Qdf %>% count(word,sort=TRUE))[1:x,]
  }
  else if(n==2){
    #manipulate the data so that each bigram has its own row
    tidy_Qdf<- text_df %>% unnest_tokens(bigram,Subject,to_lower=TRUE,token="ngrams",n=2)
    #separate bigrams into individual words
    bigrams_separated <- tidy_Qdf %>% separate(bigram, c("word1", "word2"), sep = " ")
    #remove cases where one of the words is a stopword
    bigrams_filtered <- bigrams_separated %>%
      filter(!word1 %in% stop_words$word) %>%
      filter(!word2 %in% stop_words$word)
    
    #count the occurrences of word pairs, sort by the number of occurrences, and take the top x
    top_x <- as.data.frame((bigrams_filtered %>% count(word1, word2, sort = TRUE))[1:x,])
    
    #rejoin the words back into bigrams
    top_x$phrase <- sapply(1:x,
                           function(x)
                             paste(top_x[x,]$word1,top_x[x,]$word2))
    #only keep the bigrams
    top_x <- top_x[,!(names(top_x) %in% c("word1","word2"))]
  }
  who<-rep(text_df$who[1],x)
  return(cbind(top_x,who))
}


```


##Call the functions to do the analysis for a vector of names
Write the name(s) of the people who's email you want to analyse, then it calls the functions and pushes the results out to .csv files.

TODO: The rbind process is inefficient accroding to Robin, so probably want to do this is a slightly different way to speed up in due course.

NOTE: The code requires you to input whose name you are looking for, this could be in an interface in future.


```{r}

####Actual work####

#Set up a vector of people you want to look for in your email.
sent_to<-c("Ross")

# loop around all the people calling a function to filter relevant emails then 
# send to commonWords & commonBigrams functions
for (i in 1:length(sent_to)){
        
        if (i==1){ 
                input_data<-individual_emails(raw_data, who=sent_to[i])
                commonWords <- input_data %>% mostcommon( x=20) %>% cbind(sent_to[i])
                commonBigrams <- input_data %>% mostcommon(n=2, x=20) %>% cbind(sent_to[i])
        }
        else {
                input_data<-individual_emails(raw_data, who=sent_to[i])
                commonWords <- input_data %>% mostcommon( x=20) %>% cbind(sent_to[i]) %>% rbind(commonWords)
                commonBigrams <- input_data %>% mostcommon(n=2, x=20) %>% cbind(sent_to[i]) %>% rbind(commonBigrams)
        }
}


#and write out
write.csv(commonWords,"../SensitiveData/commonWords.csv")
write.csv(commonBigrams,"../SensitiveData/commonBigrams.csv")


```

##Can we plot a few things please

This only plots for one person. 
TODO: Add functionality that plots for more than one person. Could do by splitting and selecting one person, or i guess by multiple charts...

```{r}

#plot(commonBigrams$`sent_to[i]`, commonBigrams$n)
#Well barplot is a bit sh1t. 
#barplot(commonBigrams$n, names.arg = commonBigrams$phrase, horiz = TRUE)



ggplot(commonBigrams, aes(x = phrase, y = n)) + geom_bar(stat = "identity") +
  xlab("Terms") + ylab("Count") + coord_flip()


```



